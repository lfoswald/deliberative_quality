mutate(text_deliberation = (TOXICITY+arg_l_coms+rec_n_coms)/3,
deliberation = scale(deliberation),
text_deliberation = scale(text_deliberation),
del_complexity_G = scale(del_complexity_G))
sum <- comment_data%>%
group_by(subreddit)%>%
summarise(mean_del = mean(deliberation),
se_del = sd(deliberation)/sqrt(n()),
mean_txt = mean(text_deliberation),
se_txt = sd(text_deliberation)/sqrt(n()),
mean_str = mean(del_complexity_G),
se_str = sd(del_complexity_G)/sqrt(n()),
n = n()
)
p1 <- ggplot(comment_data, aes(x=deliberation, y=text_deliberation, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_txt),size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_txt,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_txt,
ymax = mean_txt+se_txt,ymin = mean_txt-se_txt))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Textual features of deliberative quality")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p1
ggsave("output/scatter_science_text.png", width = 6, height = 5, dpi = 500, p1)
# run old plot again (using structural measures)
p2 <- ggplot(comment_data, aes(x=deliberation, y=del_complexity_G, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_str), size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_str,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_str,
ymax = mean_str+se_str,ymin = mean_str-se_str))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Structural complexity (depth x max width)")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p2
ggsave("output/scatter_science_del.png", width = 6, height = 5, dpi = 500, p2)
combi <- grid.arrange(p2, p1, nrow = 1)
ggsave("output/scatter_science_combi.png", width = 12, height = 5, dpi = 500, combi)
#### descriptives for automated measures between subreddits
# 1. Argumentation (length comments)
comment_data$arg_l_coms <- nchar(comment_data$text_c)
hist(comment_data$arg_l_coms)
mean(comment_data$arg_l_coms)
# 2. Reciprocity (N comments)
comment_data$rec_n_coms <- comment_data$n_comments_sub
hist(comment_data$rec_n_coms)
mean(comment_data$rec_n_coms)
##### Correlation Plot with all qualitative and computational measures
corr_data <- comment_data%>%
dplyr::select(del_complexity_G, gonzalez_width, max_thread_depth, arg_l_coms, rec_n_coms, TOXICITY,
respect, argumentation, reciprocity, empathy, emotion, humor)%>%
mutate_all(~as.numeric(as.character(.)))%>%
plyr::rename(c("del_complexity_G" = "Structural Complexity",
"gonzalez_width" = "Thread Width",
"max_thread_depth" = "Thread Depth",
"arg_l_coms" = "Comment Length",
"rec_n_coms" = "Number of Replies",
"TOXICITY" = "Toxicity",
"respect" = "Respect",
"argumentation" = "Argumentation",
"reciprocity"  = "Reciprocity",
"empathy" = "Empathy",
"emotion" = "Emotion",
"humor" = "Humor"
))
corr <- round(cor(corr_data, use = "pairwise.complete.obs"), 1)
p.mat <- cor_pmat(corr_data, use = "pairwise.complete.obs")
corr_c_plot <- ggcorrplot(corr, method = "circle", type = "upper",
outline.col = "white",
title = "Bivariate Correlations - Comment Level Data", insig = "blank",
tl.cex = 8,
tl.srt = 90,
lab = T, lab_size = 3, show.legend = F)
corr_c_plot
# create summary score of textual measures
comment_data <- comment_data%>% # change to "thread_data" and run code below for thread level data
mutate(text_deliberation = (TOXICITY+arg_l_coms+rec_n_coms)/3,
deliberation = scale(deliberation),
text_deliberation = scale(text_deliberation),
del_complexity_G = scale(del_complexity_G))
sum <- comment_data%>%
group_by(subreddit)%>%
summarise(mean_del = mean(deliberation),
se_del = sd(deliberation)/sqrt(n()),
mean_txt = mean(text_deliberation),
se_txt = sd(text_deliberation)/sqrt(n()),
mean_str = mean(del_complexity_G),
se_str = sd(del_complexity_G)/sqrt(n()),
n = n()
)
p1 <- ggplot(comment_data, aes(x=deliberation, y=text_deliberation, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_txt),size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_txt,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_txt,
ymax = mean_txt+se_txt,ymin = mean_txt-se_txt))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Textual features of deliberative quality")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p1
ggsave("output/scatter_science_text.png", width = 6, height = 5, dpi = 500, p1)
# date range
comment_data%>%
group_by(wave)%>%
summarize(start = min(date),
end = max(date))
# comment in-degree range
comment_data%>%
group_by(subreddit)%>%
summarize(mean = min(in_degree_com),
max = max(in_degree_com))
# N comments
comment_data%>%
group_by(subreddit, wave)%>%
summarise(n())%>%
xtable()%>%
kable()%>%
kable_styling(bootstrap_options = "striped",
full_width = FALSE,
position = "center")
# N threads
thread_data%>%
group_by(subreddit, wave)%>%
summarise(n())%>%
xtable()%>%
kable()%>%
kable_styling(bootstrap_options = "striped",
full_width = FALSE,
position = "center")
# comment level data
data <- comment_data
my.mean<-tapply(data[,"deliberation"],data[,"subreddit"],mean, na.rm =T)
my.sd<-tapply(data[,"deliberation"],data[,"subreddit"],sd)
my.nr<-as.vector(table(data[,"subreddit"]))
my.se<-my.sd/sqrt(my.nr)
my.df<-data.frame(groups=factor(names(my.mean),labels =c("climate change","climate skeptics")),
mean=my.mean,
sd=my.sd,
se=my.se)
pa <- ggplot(data, aes(y=deliberation, x=log(del_structure))) +
geom_point(colour="seagreen3")+
geom_smooth( color = "grey")+
theme_bw(base_size = 8)+
ylab("Deliberative Quality (Content Coding)")+
xlab("Structural Complexity (Network Metrics, In-Degrees)")
pb <- ggplot(data, aes(y=deliberation, x=log(del_complexity))) +
geom_point(color="seagreen3")+
geom_smooth(color = "grey")+
theme_bw(base_size = 8)+
ylab("")+
xlab("Structural Complexity (Network Metrics, Users)")
pc <- ggplot(data, aes(y=deliberation, x=log(del_complexity_G))) +
geom_point(colour="seagreen3")+
geom_smooth(color = "grey")+
theme_bw(base_size = 8)+
ylab("")+
xlab("Structural Complexity (Network Metrics, Gonzalez)")
p_pred <- grid.arrange(pa, pb, pc, nrow = 1)
m1 <- lm(scale(deliberation) ~ subreddit + opposing + opposing * subreddit, data = thread_data)
m2 <- lm(scale(del_complexity_G) ~ subreddit + opposing + opposing * subreddit, data = thread_data)
m3 <- lm(scale(del_complexity) ~ subreddit + opposing + opposing * subreddit, data = thread_data)
stargazer(m1,m2,m3, type = "text", omit = "Constant")
m5 <- lm(scale(deliberation) ~ subreddit + opposing + opposing * subreddit, data = data)
m6 <- lm(scale(del_complexity_G) ~ subreddit + opposing + opposing * subreddit, data = data)
m7 <- lm(scale(del_complexity) ~ subreddit + opposing + opposing * subreddit, data = data)
stargazer(m5,m6,m7, type = "text", omit = "Constant")
ggplot(data, aes(x=scale(deliberation), y=scale(del_complexity_G), colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data=data %>%
group_by(subreddit) %>%
summarise_at(vars("deliberation","del_complexity_G"), mean),
size=4, shape=23, fill = "black",stroke = 1.5)+
geom_smooth(data = data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("#fb9a99", "#1f78b4"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Structural complexity (depth x max width)")+
theme(legend.position = c(.88,.9))
#### Depth x Max Width scatter  ####
q <- ggplot(thread_data, aes(max_thread_depth, gonzalez_width))+
geom_point(aes(colour = as.factor(opposing), size = n_comments_sub),
position = position_jitter(width = 2, height = 0),
alpha = 0.5)+
theme_bw()+
facet_grid(vars(wave), vars(subreddit))+
xlab("Maximum Depth")+
ylab("Maximum Width")+
scale_colour_manual(name="Submission Content",
labels=c("congruent", "opposing"),
values=c("blue","red")) +
scale_size_continuous(name="Number of Comments")
q
#### Sarcasm between Subreddits #####
options(digits = 2)
means <- data %>%
group_by(subreddit)%>%
summarize(sarcasm = mean(civ_sar),
paternalism = mean(civ_pat),
comments = n())
sds <- data %>%
group_by(subreddit)%>%
summarize(sarcasm = sd(civ_sar),
paternalism = sd(civ_pat),
comments = n())
vars <- list(civ_sar, civ_pat)
var_names <- c("sarcasm","paternalism")
t <- c()
t_p <- c()
df <- c()
for(var in vars){
test = t.test(var ~ subreddit, data = data)
t = c(t, test$statistic)
df = c(df, test$parameter)
t_p = c(t_p, round(test$p.value,3))
}
ts <- data.frame(var_names, t,df, t_p)
ts[nrow(ts)+1,] <- NA
ts.T <- t(ts[,2:ncol(ts)])
colnames(ts.T) <- ts[,1]
ts <- tibble::rownames_to_column(data.frame(ts.T), "stat")
colnames(ts) <- colnames(means)
rbind(means,sds,ts)%>%
xtable()%>%
kable()%>% #kable("latex")
kable_styling(bootstrap_options = "striped",
full_width = FALSE,
position = "center")
#### Automating the Assessment of Deliberative Quality? ######
library(tidyverse)
library(readr)
library(readxl)
library(xtable)
library(knitr)
library(kableExtra)
library(stargazer)
library(writexl)
library(psych)
library(gridExtra)
comment_data <- read.csv( "data/final/comment_data_preprocessed.csv")
thread_data <- read.csv("data/final/thread_data_preprocessed.csv")
# date range
comment_data%>%
group_by(wave)%>%
summarize(start = min(date),
end = max(date))
# comment in-degree range
comment_data%>%
group_by(subreddit)%>%
summarize(mean = min(in_degree_com),
max = max(in_degree_com))
# N comments
comment_data%>%
group_by(subreddit, wave)%>%
summarise(n())%>%
xtable()%>%
kable()%>%
kable_styling(bootstrap_options = "striped",
full_width = FALSE,
position = "center")
# N threads
thread_data%>%
group_by(subreddit, wave)%>%
summarise(n())%>%
xtable()%>%
kable()%>%
kable_styling(bootstrap_options = "striped",
full_width = FALSE,
position = "center")
#### Depth x Max Width scatter  ####
q <- ggplot(thread_data, aes(max_thread_depth, gonzalez_width))+
geom_point(aes(colour = as.factor(opposing), size = n_comments_sub),
position = position_jitter(width = 2, height = 0),
alpha = 0.5)+
theme_bw()+
facet_grid(vars(wave), vars(subreddit))+
xlab("Maximum Depth")+
ylab("Maximum Width")+
scale_colour_manual(name="Submission Content",
labels=c("congruent", "opposing"),
values=c("blue","red")) +
scale_size_continuous(name="Number of Comments")
q
#ggsave(file="output/G_width_depth.pdf",width = 8, height = 6, dpi = 500, q)
#### Depth x Max Width scatter  ####
q <- ggplot(thread_data, aes(max_thread_depth, gonzalez_width))+
geom_point(aes(colour = as.factor(opposing), size = n_comments_sub),
position = position_jitter(width = 2, height = 0),
alpha = 0.5)+
theme_bw()+
facet_grid(vars(wave), vars(subreddit))+
xlab("Maximum Depth")+
ylab("Maximum Width")+
scale_colour_manual(name="Submission Content",
labels=c("congruent", "opposing"),
values=c("blue","red")) +
scale_size_continuous(name="Number of Comments")
q
# create summary score of textual measures
comment_data <- comment_data%>% # change to "thread_data" and run code below for thread level data
mutate(text_deliberation = (TOXICITY+arg_l_coms+rec_n_coms)/3,
deliberation = scale(deliberation),
text_deliberation = scale(text_deliberation),
del_complexity_G = scale(del_complexity_G))
sum <- comment_data%>%
group_by(subreddit)%>%
summarise(mean_del = mean(deliberation),
se_del = sd(deliberation)/sqrt(n()),
mean_txt = mean(text_deliberation),
se_txt = sd(text_deliberation)/sqrt(n()),
mean_str = mean(del_complexity_G),
se_str = sd(del_complexity_G)/sqrt(n()),
n = n()
)
p1 <- ggplot(comment_data, aes(x=deliberation, y=text_deliberation, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_txt),size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_txt,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_txt,
ymax = mean_txt+se_txt,ymin = mean_txt-se_txt))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Textual features of deliberative quality")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p1
######### Automating the Analysis of Online Deliberation #############
library(peRspective)
library(tidyverse)
library(readr)
library(readxl)
library(xtable)
library(knitr)
library(kableExtra)
library(stargazer)
library(writexl)
library(psych)
library(gridExtra)
library(ggcorrplot)
comment_data <- read.csv( "data/final/comment_data_preprocessed.csv")
thread_data <- read.csv("data/final/thread_data_preprocessed.csv")
# Level of analysis: Threads
# Adding alternative measures for
# 1. Argumentation (length comments)
comment_data$arg_l_coms <- nchar(comment_data$text_c)
hist(comment_data$arg_l_coms)
mean(comment_data$arg_l_coms)
# 2. Reciprocity (N comments)
comment_data$rec_n_coms <- comment_data$n_comments_sub
hist(comment_data$rec_n_coms)
mean(comment_data$rec_n_coms)
# 3. Respect (Toxicity) using Perspective API (Google)
# https://github.com/favstats/peRspective
#usethis::edit_r_environ()
# add:  perspective_api_key="AIzaSyB0dABPIS0t0GkVKJM7mZlG8n4HDmtCcs8"
#test
#prsp_score(comment_data$text_c[1], languages = "en",
#           score_model = "TOXICITY", doNotStore = TRUE)
#big run
#tox_data <- comment_data%>%
#  prsp_stream(text = text_c,
#              text_id = id_c,
#              score_model = c("TOXICITY", "SEVERE_TOXICITY"),
#              safe_output = T,
#              languages = "en",
#              doNotStore = TRUE)
#
#write_xlsx(tox_data, "data/temp/tox_data.xlsx")
tox_data <- read_excel("data/temp/tox_data.xlsx")
tox_data$id_c <- tox_data$text_id
comment_data_tox <- left_join(comment_data, tox_data, by.y = "id_c")
hist(comment_data_tox$TOXICITY)
comment_data_tox%>%
arrange(desc(TOXICITY))%>%
select(text_c)%>%
head() # pretty accurate haha
# add scores to thread data
thread_data_tox <- comment_data_tox%>%
dplyr::group_by(id_sub) %>%
mutate(TOXICITY = mean(TOXICITY),
arg_l_coms = mean(arg_l_coms),
rec_n_coms = mean(rec_n_coms))%>%
slice(1)
write.csv(comment_data_tox, file = "data/final/comment_data_tox.csv")
write.csv(thread_data_tox, file = "data/final/thread_data_tox.csv")
########### multiple models ###########
comment_data <- read.csv( "data/final/comment_data_tox.csv")
thread_data <- read.csv("data/final/thread_data_tox.csv")
mod1 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width), data = comment_data)
mod2 <- lm(deliberation ~ scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms), data = comment_data)
mod3 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width), data = thread_data)
mod4 <- lm(deliberation ~ scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms), data = thread_data)
mod5 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width)+ scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms), data = comment_data)
mod6 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width)+ scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms), data = thread_data)
mod7 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width)+scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms) + opposing, data = comment_data)
mod8 <- lm(deliberation ~ scale(max_thread_depth) + scale(gonzalez_width)+scale(log(arg_l_coms)) + scale(TOXICITY) + scale(rec_n_coms) + opposing, data = thread_data)
# comment level data
stargazer(mod1, mod2, mod5, mod7, type = "html", omit = "Constant", out = "table1.html")
#thread level data
stargazer(mod3, mod4, mod6, mod8, type = "html", omit = "Constant", out = "table2.html")
##### Correlation Plot with all qualitative and computational measures
corr_data <- comment_data%>%
dplyr::select(del_complexity_G, gonzalez_width, max_thread_depth, arg_l_coms, rec_n_coms, TOXICITY,
respect, argumentation, reciprocity, empathy, emotion, humor)%>%
mutate_all(~as.numeric(as.character(.)))%>%
plyr::rename(c("del_complexity_G" = "Structural Complexity",
"gonzalez_width" = "Thread Width",
"max_thread_depth" = "Thread Depth",
"arg_l_coms" = "Comment Length",
"rec_n_coms" = "Number of Replies",
"TOXICITY" = "Toxicity",
"respect" = "Respect",
"argumentation" = "Argumentation",
"reciprocity"  = "Reciprocity",
"empathy" = "Empathy",
"emotion" = "Emotion",
"humor" = "Humor"
))
corr <- round(cor(corr_data, use = "pairwise.complete.obs"), 1)
p.mat <- cor_pmat(corr_data, use = "pairwise.complete.obs")
corr_c_plot <- ggcorrplot(corr, method = "circle", type = "upper",
outline.col = "white",
title = "Bivariate Correlations - Comment Level Data", insig = "blank",
tl.cex = 8,
tl.srt = 90,
lab = T, lab_size = 3, show.legend = F)
corr_c_plot
#ggsave("output/corr_comments_plot.pdf", width = 6, height = 5, dpi = 300, corr_c_plot)
### Scatterplot with textual measures between subreddits
# create summary score of textual measures
comment_data <- comment_data%>% # change to "thread_data" and run code below for thread level data
mutate(text_deliberation = (TOXICITY+arg_l_coms+rec_n_coms)/3,
deliberation = scale(deliberation),
text_deliberation = scale(text_deliberation),
del_complexity_G = scale(del_complexity_G))
sum <- comment_data%>%
group_by(subreddit)%>%
summarise(mean_del = mean(deliberation),
se_del = sd(deliberation)/sqrt(n()),
mean_txt = mean(text_deliberation),
se_txt = sd(text_deliberation)/sqrt(n()),
mean_str = mean(del_complexity_G),
se_str = sd(del_complexity_G)/sqrt(n()),
n = n()
)
p1 <- ggplot(comment_data, aes(x=deliberation, y=text_deliberation, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_txt),size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_txt,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_txt,
ymax = mean_txt+se_txt,ymin = mean_txt-se_txt))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Textual features of deliberative quality")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p1
#ggsave("output/scatter_science_text.png", width = 6, height = 5, dpi = 500, p1)
# run old plot again (using structural measures)
p2 <- ggplot(comment_data, aes(x=deliberation, y=del_complexity_G, colour = subreddit)) +
geom_point(alpha = 0.1)+
#geom_point(data=thread_data, alpha = 0.3, size=5)+
geom_point(data = sum, aes(x=mean_del, y=mean_str), size = 3)+
geom_errorbarh(data = sum, aes(x=mean_del, y=mean_str,
xmax = mean_del+se_del, xmin = mean_del-se_del))+
geom_errorbar(data = sum, aes(x=mean_del, y=mean_str,
ymax = mean_str+se_str,ymin = mean_str-se_str))+
geom_smooth(data = comment_data, method = lm,  size = 0.5, color = "black",alpha = 0.3)+
theme_bw()+
scale_colour_manual(values = c("blue", "red"))+
labs(colour ="Subreddit Mean",
x = "Deliberative quality (content coding)",
y = "Structural complexity (depth x max width)")+
theme(legend.position = c(.88,.9))+
ylim(-1, 3)
p2
#ggsave("output/scatter_science_del.png", width = 6, height = 5, dpi = 500, p2)
combi <- grid.arrange(p2, p1, nrow = 1)
#ggsave("output/scatter_science_combi.png", width = 12, height = 5, dpi = 500, combi)
