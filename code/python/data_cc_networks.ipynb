{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection - Wildfires & Climate Change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- developer: name\n",
    "- personal-use-script: A\n",
    "- secret: ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import networkx as nx \n",
    "import pickle\n",
    "import pydot\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='A',\n",
    "                     client_secret='ABC',\n",
    "                     user_agent='name',\n",
    "                     username= 'name', \n",
    "                     password= 'name'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = [\n",
    " 'climatechange',\n",
    " 'climateskeptics'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Dataframe for multiple Subreddits\n",
    "\n",
    "* Iterating through a list of subreddits\n",
    "* Iterating through unlimmited number of hot submissions that contain the keyword \"fire\" in each subreddit\n",
    "* Getting the comments for each submission\n",
    "* Saving the author (if not deleted)\n",
    "* Saving the comment body (if not deleted)\n",
    "* Append dataframe-list with comment id, body, author name, upvotes, timestamp, comment level (depth) and parent id and subreddit name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for c, submission in enumerate(subreddit.hot(limit=None)):\n",
    "        if \"fire\" in submission.title:\n",
    "            for c,comment in enumerate(submission.comments.list()):\n",
    "                # THe try exists because some reddit comments are from authors who \n",
    "                # have deleted their account, but the comments persist. \n",
    "                try:\n",
    "                    x = comment.author.name,\n",
    "                    authorname = x[0]\n",
    "                except AttributeError:\n",
    "                    authorname = \"[deleted]\"\n",
    "            \n",
    "                try:\n",
    "                    comment_body = comment.body,\n",
    "                except AttributeError:\n",
    "                    comment_body = \"[deleted]\"\n",
    "            \n",
    "                try: \n",
    "                    df_list.append([ \\\n",
    "                    comment.id,\n",
    "                    comment_body,\n",
    "                    authorname,\n",
    "                    comment.ups,\n",
    "                    comment.created_utc,\n",
    "                    comment.depth,\n",
    "                    comment.parent_id[3:],\n",
    "                    subreddit\n",
    "                    ])\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "                    \n",
    "reddit_df = pd.DataFrame(df_list,columns=[\"id\",\"body\",\"authorname\",\"ups\",\"created_utc\",\"depth\",\"parent_id\",\"subreddit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime object\n",
    "reddit_df[\"date\"] = reddit_df[\"created_utc\"].map(lambda x: datetime.utcfromtimestamp(x))\n",
    "\n",
    "display(reddit_df.head(10))\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this in April 2020 for Australian Wildfires and in October 2020 for Californian Wildfires (With same keyword settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('fire_reddit_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(reddit_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# also save datafrate as csv \n",
    "reddit_df.to_csv('fire_reddit_comments_df.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create comment tree (acyclic graph) for each subreddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sub_list:\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    node_set = set([])\n",
    "    \n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    \n",
    "    for submission in subreddit.hot(limit=None):\n",
    "        if \"fire\" in submission.title:\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            \n",
    "            for i in submission.comments.list():\n",
    "                try:\n",
    "                    G.add_node(i.id, depth=i.depth,name=i.author.name)\n",
    "                except AttributeError: \n",
    "                    G.add_node(i.id, depth=i.depth,name=\"[Deleted]\")\n",
    "                node_set.add(i.id)\n",
    "            \n",
    "            for i in submission.comments.list():\n",
    "                if i.parent_id and i.parent_id[3:] in node_set:\n",
    "                    G.add_edge(i.id,i.parent_id[3:])\n",
    "    \n",
    "    print(subreddit,G.number_of_nodes(),'nodes')\n",
    "\n",
    "    nx.write_gexf(G,('%s.gexf'%subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Australia and California Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reddit_df for Australia and California as pickles\n",
    "with open('aus_fire_reddit_df.pickle', 'rb') as handle:\n",
    "    reddit_df_aus = pickle.load(handle)\n",
    "\n",
    "with open('cal_fire_reddit_df.pickle', 'rb') as handle:\n",
    "    reddit_df_cal = pickle.load(handle)\n",
    "\n",
    "\n",
    "reddit_df_aus['wave'] = 'australia'  \n",
    "reddit_df_cal['wave'] = 'california' \n",
    "        \n",
    "reddit_df = reddit_df_aus.append(reddit_df_cal)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('reddit_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(reddit_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# save datafrate as csv \n",
    "reddit_df.to_csv('reddit_df.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find original submission to comment trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_df.pickle', 'rb') as handle:\n",
    "    reddit_df = pickle.load(handle)\n",
    "    \n",
    "display(reddit_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climatechange Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gexf(\"climatechange_aus.gexf\")\n",
    "    \n",
    "threads = pd.Series([g.subgraph(c) for c in nx.weakly_connected_component_subgraphs(g)])\n",
    "    \n",
    "subreddit_threads = []\n",
    "all_trigger_subs_df = []\n",
    "    \n",
    "for c,g in enumerate(threads):\n",
    "    \n",
    "    comment_branch = list(nx.dag_longest_path(g))\n",
    "    lpcb_df = reddit_df[reddit_df[\"id\"].isin(comment_branch)]\n",
    "    temp_sorted = lpcb_df.sort_values(\"depth\",ascending=True)    \n",
    "    \n",
    "    count = 0\n",
    "    for index,row in temp_sorted.iterrows():\n",
    "        if count < 1:\n",
    "            c = row.parent_id                         \n",
    "            subreddit_threads.append(c)\n",
    "            count += 1\n",
    "            \n",
    "    # Run another API call to get trigger submissions    \n",
    "    \n",
    "    subs = {}    \n",
    "        \n",
    "    for cs in subreddit_threads:\n",
    "        s = reddit.submission(id = '{}'.format(cs))\n",
    "        subs[cs] = (s.id, s.title, s.author, s.ups, s.num_comments)    \n",
    "        cs_df = pd.DataFrame(subs, index=[\"id\",\"title\",\"author\",\"ups_sub\", \"n_comments_sub\"]).T\n",
    "        all_trigger_subs_df.append(cs_df)\n",
    "\n",
    "all_trigger_subs_df = pd.concat(all_trigger_subs_df) \n",
    "\n",
    "display(all_trigger_subs_df)\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('trigger_climatechange_aus.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_trigger_subs_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# write to csv\n",
    "all_trigger_subs_df.to_csv(r'trigger_climatechange_aus.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climateskeptics Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = nx.read_gexf(\"climateskeptics_aus.gexf\")\n",
    "    \n",
    "threads = pd.Series([g.subgraph(c) for c in nx.weakly_connected_component_subgraphs(g)])\n",
    "    \n",
    "subreddit_threads = []\n",
    "all_trigger_subs_df = []\n",
    "    \n",
    "for c,g in enumerate(threads):\n",
    "    \n",
    "    comment_branch = list(nx.dag_longest_path(g))\n",
    "    lpcb_df = reddit_df[reddit_df[\"id\"].isin(comment_branch)]\n",
    "    temp_sorted = lpcb_df.sort_values(\"depth\",ascending=True)    \n",
    "    \n",
    "    count = 0\n",
    "    for index,row in temp_sorted.iterrows():\n",
    "        if count < 1:\n",
    "            c = row.parent_id                         \n",
    "            subreddit_threads.append(c)\n",
    "            count += 1\n",
    "            \n",
    "    # Run another API call to get trigger submissions    \n",
    "    \n",
    "    subs = {}    \n",
    "        \n",
    "    for cs in subreddit_threads:\n",
    "        s = reddit.submission(id = '{}'.format(cs))\n",
    "        subs[cs] = (s.id, s.title, s.author, s.ups, s.num_comments)    \n",
    "        cs_df = pd.DataFrame(subs, index=[\"id\",\"title\",\"author\",\"ups_sub\", \"n_comments_sub\"]).T\n",
    "        all_trigger_subs_df.append(cs_df)\n",
    "\n",
    "all_trigger_subs_df = pd.concat(all_trigger_subs_df) \n",
    "\n",
    "display(all_trigger_subs_df)\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('trigger_climateskeptics_aus.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_trigger_subs_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# write to csv\n",
    "all_trigger_subs_df.to_csv(r'trigger_climateskeptics_aus.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climatechange California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = nx.read_gexf(\"climatechange_cal.gexf\")\n",
    "    \n",
    "threads = pd.Series([g.subgraph(c) for c in nx.weakly_connected_component_subgraphs(g)])\n",
    "    \n",
    "subreddit_threads = []\n",
    "all_trigger_subs_df = []\n",
    "    \n",
    "for c,g in enumerate(threads):\n",
    "    \n",
    "    comment_branch = list(nx.dag_longest_path(g))\n",
    "    lpcb_df = reddit_df[reddit_df[\"id\"].isin(comment_branch)]\n",
    "    temp_sorted = lpcb_df.sort_values(\"depth\",ascending=True)    \n",
    "    \n",
    "    count = 0\n",
    "    for index,row in temp_sorted.iterrows():\n",
    "        if count < 1:\n",
    "            c = row.parent_id                         \n",
    "            subreddit_threads.append(c)\n",
    "            count += 1\n",
    "            \n",
    "    # Run another API call to get trigger submissions    \n",
    "    \n",
    "    subs = {}    \n",
    "        \n",
    "    for cs in subreddit_threads:\n",
    "        s = reddit.submission(id = '{}'.format(cs))\n",
    "        subs[cs] = (s.id, s.title, s.author, s.ups, s.num_comments)    \n",
    "        cs_df = pd.DataFrame(subs, index=[\"id\",\"title\",\"author\",\"ups_sub\", \"n_comments_sub\"]).T\n",
    "        all_trigger_subs_df.append(cs_df)\n",
    "\n",
    "all_trigger_subs_df = pd.concat(all_trigger_subs_df) \n",
    "\n",
    "display(all_trigger_subs_df)\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('trigger_climatechange_cal.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_trigger_subs_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# write to csv\n",
    "all_trigger_subs_df.to_csv(r'trigger_climatechange_cal.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climateskeptics California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gexf(\"climateskeptics_cal.gexf\")\n",
    "    \n",
    "threads = pd.Series([g.subgraph(c) for c in nx.weakly_connected_component_subgraphs(g)])\n",
    "    \n",
    "subreddit_threads = []\n",
    "all_trigger_subs_df = []\n",
    "    \n",
    "for c,g in enumerate(threads):\n",
    "    \n",
    "    comment_branch = list(nx.dag_longest_path(g))\n",
    "    lpcb_df = reddit_df[reddit_df[\"id\"].isin(comment_branch)]\n",
    "    temp_sorted = lpcb_df.sort_values(\"depth\",ascending=True)    \n",
    "    \n",
    "    count = 0\n",
    "    for index,row in temp_sorted.iterrows():\n",
    "        if count < 1:\n",
    "            c = row.parent_id                         \n",
    "            subreddit_threads.append(c)\n",
    "            count += 1\n",
    "            \n",
    "    # Run another API call to get trigger submissions    \n",
    "    \n",
    "    subs = {}    \n",
    "        \n",
    "    for cs in subreddit_threads:\n",
    "        s = reddit.submission(id = '{}'.format(cs))\n",
    "        subs[cs] = (s.id, s.title, s.author, s.ups, s.num_comments)    \n",
    "        cs_df = pd.DataFrame(subs, index=[\"id\",\"title\",\"author\",\"ups_sub\", \"n_comments_sub\"]).T\n",
    "        all_trigger_subs_df.append(cs_df)\n",
    "\n",
    "all_trigger_subs_df = pd.concat(all_trigger_subs_df) \n",
    "\n",
    "display(all_trigger_subs_df)\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('trigger_climateskeptics_cal.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_trigger_subs_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# write to csv\n",
    "all_trigger_subs_df.to_csv(r'trigger_climateskeptics_cal.csv', index = True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
